{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoord Quick Start\n",
    "\n",
    "With this notebook, you can get started with DeepCoord quickly.\n",
    "\n",
    "You will set up and run DeepCoord, observe its training curves over time, and analyze some results.\n",
    "\n",
    "Please note that this is just meant to get you started - you will likely have to adjust and extend this code to achieve what you want.\n",
    "\n",
    "## Test and Run DeepCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ensure deepcoord is installed properly --> should show available CLI options\n",
    "# if this fails check your installation; run \"pip install -r requirements.txt\"\n",
    "# ignore TensorFlow warnings\n",
    "\n",
    "!deepcoord -h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitor Training Progress on TensorBoard\n",
    "\n",
    "While DeepCoord is training, you can monitor its progress and view its learning curves on TensorBoard.\n",
    "\n",
    "DeepCoord saves TensorBoard-related graph files in the `graph` directory, which is created automatically, when you run DeepCoord.\n",
    "\n",
    "To run TensorBoard, simply run `tensorboard --logdir graph` in a separate terminal and open your browser at [localhost:6006](localhost:6006) (recommended).\n",
    "\n",
    "Alternatively, you can also run TensorBoard in a Jupyter notebook:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from operator import itemgetter\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx\n",
    "%matplotlib inline\n",
    "\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "# prefix for saving plot files\n",
    "plot_prefix = 'var-node-cap_mmpp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted version\n",
    "def read_results_over_networks(dir, df=None, rf=None, best_only=False, network_dir=None, skip_rf=True, alg_name=None):\n",
    "    \"\"\"Read result files in the directory and append results to the data frame\"\"\"\n",
    "    if df is None:\n",
    "        df = pd.DataFrame()\n",
    "    if rf is None:\n",
    "        rf = pd.DataFrame()\n",
    "\n",
    "    if network_dir is not None:\n",
    "        dir = f'{network_dir}/{dir}'\n",
    "    \n",
    "    for root, sub_dirs, files in os.walk(dir):\n",
    "        if len(files) > 0 and (not '.ipynb_checkpoints' in root):                \n",
    "            if 'best' in root or not best_only:\n",
    "                # read metrics at end of simulation (last row)\n",
    "                df_metrics = pd.read_csv(f'{root}/metrics.csv')\n",
    "                data = df_metrics.tail(1)\n",
    "                # get the last placement\n",
    "                df_placements = pd.read_csv(f'{root}/placements.csv').groupby('time')\n",
    "                placements = pd.concat(deque(map(itemgetter(1), df_placements), maxlen=1))\n",
    "                \n",
    "                \n",
    "                # add algorithm name, number of ingress nodes, and number of vnfs as columns\n",
    "                with open(f'{root}/input.yaml', 'r') as f:\n",
    "                    inputs = yaml.load(f, Loader=yaml.Loader)\n",
    "                if alg_name is None:\n",
    "                    alg_name = inputs['algorithm']\n",
    "                \n",
    "                # read number of nodes in network\n",
    "                network_file = [filename for filename in files if \".graphml\" in filename]\n",
    "                assert len(network_file) == 1\n",
    "                network_file = network_file[0]\n",
    "                if network_dir is None:\n",
    "                    network_dir = network_file.split('.')[0]\n",
    "                nx_network = networkx.read_graphml(f'{root}/{network_file}')\n",
    "                num_nodes = nx_network.number_of_nodes()\n",
    "                network_cap = sum([n[1].get('NodeCap') for n in nx_network.nodes(data=True)])\n",
    "                \n",
    "                data.insert(loc=0, column='network', value=network_dir)\n",
    "                data.insert(loc=1, column='network_cap', value=network_cap)\n",
    "                data.insert(loc=2, column='algorithm', value=alg_name)\n",
    "                data.insert(loc=3, column='num_nodes', value=num_nodes)\n",
    "                data.insert(loc=4, column='num_ingress', value=inputs['num_ingress'])\n",
    "                data.insert(loc=data.columns.size, column='num_vnfs', value=placements['sf'].size)\n",
    "                \n",
    "                df = df.append(data, ignore_index=True, sort=True)\n",
    "                fix_dtypes(df)\n",
    "                df.sort_values(['algorithm', 'num_nodes'], inplace=True)\n",
    "                df = df[['network', 'algorithm', 'num_nodes', 'num_ingress', 'network_cap', 'total_flows', 'successful_flows', 'dropped_flows', 'in_network_flows', 'avg_end2end_delay', 'num_vnfs']]\n",
    "\n",
    "                if not skip_rf:\n",
    "                    # Reading resource metrics and creating a new Resource Dataframe\n",
    "                    # Getting the last group using the Deque operation\n",
    "                    rf_resources = pd.read_csv(f'{root}/node_metrics.csv').groupby('time')\n",
    "                    resources = pd.concat(deque(map(itemgetter(1), rf_resources), maxlen=1))\n",
    "                    resources.insert(loc=0, column='network', value=network_dir)\n",
    "                    resources.insert(loc=1, column='network_cap', value=network_cap)\n",
    "                    resources.insert(loc=2, column='algorithm', value=inputs['algorithm'])\n",
    "                    resources.insert(loc=3, column='num_nodes', value=num_nodes)\n",
    "                    resources.insert(loc=4, column='num_ingress', value=inputs['num_ingress'])\n",
    "                    rf = rf.append(resources, ignore_index=True, sort=True)\n",
    "                    rf.sort_values(['algorithm', 'num_nodes'], inplace=True)\n",
    "                \n",
    "    #Add percentage of successful flows\n",
    "    succ_percentage = []\n",
    "    for index, row in df.iterrows():\n",
    "        succ_percentage.append(row['successful_flows'] / row['total_flows'] * 100)\n",
    "    df['successful_percentage'] = succ_percentage\n",
    "    return df, rf\n",
    "\n",
    "\n",
    "def fix_dtypes(df):\n",
    "    \"\"\"Set correct dtypes and order\"\"\"\n",
    "    df['avg_end2end_delay'] = pd.to_numeric(df['avg_end2end_delay'])\n",
    "    df['total_flows'] = pd.to_numeric(df['total_flows'])\n",
    "    df['successful_flows'] = pd.to_numeric(df['successful_flows'])\n",
    "    df['dropped_flows'] = pd.to_numeric(df['dropped_flows'])\n",
    "    df['in_network_flows'] = pd.to_numeric(df['in_network_flows'])\n",
    "    \n",
    "    \n",
    "def df_mean_std(df, group_by=['network', 'algorithm', 'num_nodes', 'num_ingress' , 'network_cap']):\n",
    "    \"\"\"Return 2 new dfs with 1) average and 2) std values\"\"\"\n",
    "    df_mean = df.groupby(group_by).mean().reset_index()\n",
    "    df_std = df.groupby(group_by).std().reset_index()\n",
    "    return df_mean, df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read results\n",
    "df = pd.DataFrame()\n",
    "rf = pd.DataFrame()\n",
    "\n",
    "df, rf = read_results_over_networks('drl', df, rf, best_only=True, alg_name='DRL')\n",
    "df, rf = read_results_over_networks('bsp', df, rf, best_only=False, alg_name='BSP')\n",
    "df, rf = read_results_over_networks('bsp-ad', df, rf, best_only=False, alg_name='BSP Ad.')\n",
    "df, rf = read_results_over_networks('sp', df, rf, best_only=False, alg_name='SP')\n",
    "df, rf = read_results_over_networks('lb', df, rf, best_only=False, alg_name='LB')\n",
    "\n",
    "df_mean, df_std = df_mean_std(df)\n",
    "# only select results with 4 ingress nodes\n",
    "df_mean = df_mean[df_mean['num_ingress'] == 4]\n",
    "df_std = df_std[df_std['num_ingress'] == 4]\n",
    "\n",
    "df_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successful Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.1, style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_successful_flows(df_mean, df_std, filename=None):\n",
    "    algs=['DRL', 'BSP','BSP Ad.', 'SP', 'LB']\n",
    "    sns.set(font_scale=1, style='white')\n",
    "    markers = ['^', 'v', 's', 'D', '<', '>', 'P', 'X', 'o', '8', 'p']\n",
    "    fig, ax1 = plt.subplots(figsize=(4, 4))\n",
    "    sns.set(font_scale=1.1, style='white')\n",
    "\n",
    "    # plot successful flows\n",
    "    for i, algorithm in enumerate(algs):\n",
    "        alg_df_mean = df_mean[df_mean['algorithm'] == algorithm]\n",
    "        alg_df_std = df_std[df_std['algorithm'] == algorithm]\n",
    "        ax1.errorbar(alg_df_mean['network_cap'], alg_df_mean['successful_percentage'], yerr=alg_df_std['successful_percentage'], capsize=5,\n",
    "                    label='{}'.format(algorithm), marker=markers[i])\n",
    "\n",
    "    # set axis, title, legend\n",
    "    # ax1.set_title('Successful Flows')\n",
    "    ax1.set_xlabel('Total Node Capacity')\n",
    "    ax1.set_ylabel('Successful Flows [%]')\n",
    "    ax1.set_ylim(0, 105)\n",
    "\n",
    "    ax1.tick_params(axis='both', direction='inout', length=5, bottom=True, left=True, right=True, top=True)\n",
    "\n",
    "    # remove error bars from legend: https://stackoverflow.com/a/15551976/2745116\n",
    "    # get handles\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    # remove the errorbars\n",
    "    handles = [h[0] for h in handles]\n",
    "    # use them in the legend\n",
    "    ax1.legend(handles, labels, numpoints=1)\n",
    "\n",
    "    # ax1.legend()\n",
    "    if filename is not None:\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(f'plots/{filename}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_successful_flows(df_mean, df_std)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}